{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "022f2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import context_precision, faithfulness, answer_correctness\n",
    "from ragas import evaluate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36be27e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total usable projects: 6762\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "collection = chroma_client.get_collection(name=\"isbsg_projects\")\n",
    "\n",
    "# Fetch all items\n",
    "all_projects = collection.get(include=[\"embeddings\", \"documents\", \"metadatas\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"id\": all_projects[\"ids\"],\n",
    "    \"embedding\": [np.array(e) for e in all_projects[\"embeddings\"]],\n",
    "    \"project_text\": all_projects[\"documents\"],\n",
    "    **{key: [md.get(key, None) for md in all_projects[\"metadatas\"]] for key in all_projects[\"metadatas\"][0].keys()}\n",
    "})\n",
    "\n",
    "# Drop projects with missing effort\n",
    "df = df[df[\"Normalised Work Effort\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total usable projects: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7fe9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval  projects: 5409\n",
      "Evaluation projects: 1353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "retrieval_df, evaluation_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save evaluation and retrieval splits (for resuming later)\n",
    "retrieval_df.to_csv(\"retrieval_projects.csv\", index=False)\n",
    "evaluation_df.to_csv(\"evaluation_projects.csv\", index=False)\n",
    "\n",
    "print(f\"Retrieval  projects: {len(retrieval_df)}\")\n",
    "print(f\"Evaluation projects: {len(evaluation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88fb1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effort_estimation(prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"o1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1dd21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_actual_effort_line(text):\n",
    "    pattern = r\"The normalized work effort for the project was [\\d\\.]+ person-hours\\.\\s*\"\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d00c3e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1353/1353 [00:00<00:00, 10309.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation collection complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Checkpoint files\n",
    "efforts_file = \"estimated_efforts.json\"\n",
    "contexts_file = \"contexts.json\"\n",
    "\n",
    "# Load progress\n",
    "estimated_efforts = json.load(open(efforts_file)) if os.path.exists(efforts_file) else {}\n",
    "contexts_dict = json.load(open(contexts_file)) if os.path.exists(contexts_file) else {}\n",
    "\n",
    "# Reload Chroma collection\n",
    "collection = chroma_client.get_collection(name=\"isbsg_projects\")\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "for idx in tqdm(range(len(evaluation_df))):\n",
    "    project_id = evaluation_df.iloc[idx][\"id\"]\n",
    "\n",
    "    # Skip if already done\n",
    "    if project_id in estimated_efforts:\n",
    "        continue\n",
    "\n",
    "    # Step 1: Embed evaluation project\n",
    "    complete_query_text = evaluation_df.iloc[idx][\"project_text\"]\n",
    "    query_text = remove_actual_effort_line(complete_query_text)\n",
    "    query_embedding = evaluation_df.iloc[idx][\"embedding\"]\n",
    "\n",
    "    # Step 2: Query top-5 similar projects\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    top_5_docs = results[\"documents\"][0]\n",
    "\n",
    "    # Save contexts\n",
    "    contexts_dict[project_id] = top_5_docs\n",
    "\n",
    "    # Step 3: Construct prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an expert software project estimation assistant. Based on the following information, estimate the effort in person-hours for the new software project.\n",
    "\n",
    "New Project Description:\n",
    "{query_text}\n",
    "\n",
    "Similar Past Project Descriptions:\"\"\"\n",
    "    for i, desc in enumerate(top_5_docs):\n",
    "        prompt += f\"\\nProject {i+1}:\\n{desc}\"\n",
    "\n",
    "    prompt += \"\\nPlease provide the estimated effort in person-hours for the new project. Format your answer exactly like:\\nThe effort for that project will be XXX person-hours.\"\n",
    "\n",
    "    # Step 4: Estimate effort\n",
    "    try:\n",
    "        effort_response = get_effort_estimation(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"API failed at index {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "    estimated_efforts[project_id] = effort_response\n",
    "\n",
    "    # Save progress\n",
    "    with open(efforts_file, \"w\") as f:\n",
    "        json.dump(estimated_efforts, f)\n",
    "    with open(contexts_file, \"w\") as f:\n",
    "        json.dump(contexts_dict, f)\n",
    "\n",
    "    # Wait to avoid rate limit\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"Evaluation collection complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df1a7ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final samples for evaluation: 1353\n"
     ]
    }
   ],
   "source": [
    "data_samples = {\n",
    "    'question': [],\n",
    "    'answer': [],\n",
    "    'contexts': [],\n",
    "    'ground_truth': []\n",
    "}\n",
    "\n",
    "for idx in range(len(evaluation_df)):\n",
    "    row = evaluation_df.iloc[idx]\n",
    "    project_id = row[\"id\"]\n",
    "\n",
    "    if project_id not in estimated_efforts or project_id not in contexts_dict:\n",
    "        continue\n",
    "\n",
    "    project_text = remove_actual_effort_line(row[\"project_text\"])\n",
    "    actual_effort_str = f\"The effort for that project will be {int(row['Normalised Work Effort'])} person-hours.\"\n",
    "    contexts = contexts_dict[project_id]\n",
    "    llm_response = estimated_efforts[project_id]\n",
    "\n",
    "    data_samples[\"question\"].append(project_text)\n",
    "    data_samples[\"answer\"].append(llm_response)\n",
    "    data_samples[\"contexts\"].append(contexts)\n",
    "    data_samples[\"ground_truth\"].append(actual_effort_str)\n",
    "\n",
    "print(f\"Final samples for evaluation: {len(data_samples['question'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "407e1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using RAGAS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4059/4059 [32:27<00:00,  2.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation Scores:\n",
      "                                             user_input  \\\n",
      "0     A Government industry project was developed by...   \n",
      "1     A Communication industry project was developed...   \n",
      "2     A Insurance industry project was developed by ...   \n",
      "3     A Government industry project was developed by...   \n",
      "4     A Communication industry project was developed...   \n",
      "...                                                 ...   \n",
      "1348  A Communication industry project was developed...   \n",
      "1349  A Services industry project was developed by a...   \n",
      "1350  A Government industry project was developed by...   \n",
      "1351  A Communication industry project was developed...   \n",
      "1352  A Medical & Health Care industry project was d...   \n",
      "\n",
      "                                     retrieved_contexts  \\\n",
      "0     [A Government industry project was developed b...   \n",
      "1     [A Communication industry project was develope...   \n",
      "2     [A Insurance industry project was developed by...   \n",
      "3     [A Government industry project was developed b...   \n",
      "4     [A Communication industry project was develope...   \n",
      "...                                                 ...   \n",
      "1348  [A Communication industry project was develope...   \n",
      "1349  [A Services industry project was developed by ...   \n",
      "1350  [A Government industry project was developed b...   \n",
      "1351  [A Communication industry project was develope...   \n",
      "1352  [A Medical & Health Care industry project was ...   \n",
      "\n",
      "                                               response  \\\n",
      "0     The effort for that project will be 2195 perso...   \n",
      "1     The effort for that project will be 3780.45904...   \n",
      "2     The effort for that project will be **160 pers...   \n",
      "3     The effort for that project will be **2648** p...   \n",
      "4     The effort for that project will be 3,364 pers...   \n",
      "...                                                 ...   \n",
      "1348  The effort for that project will be **8,044 pe...   \n",
      "1349  The effort for that project will be **7,155** ...   \n",
      "1350  The effort for that project will be 4174 perso...   \n",
      "1351  The effort for that project will be 3780 perso...   \n",
      "1352  The effort for that project will be 918 person...   \n",
      "\n",
      "                                              reference  context_precision  \\\n",
      "0     The effort for that project will be 2195 perso...                1.0   \n",
      "1     The effort for that project will be 3780 perso...                1.0   \n",
      "2     The effort for that project will be 130 person...                1.0   \n",
      "3     The effort for that project will be 2462 perso...                1.0   \n",
      "4     The effort for that project will be 3364 perso...                1.0   \n",
      "...                                                 ...                ...   \n",
      "1348  The effort for that project will be 8044 perso...                1.0   \n",
      "1349  The effort for that project will be 8478 perso...                1.0   \n",
      "1350  The effort for that project will be 4174 perso...                1.0   \n",
      "1351  The effort for that project will be 3780 perso...                1.0   \n",
      "1352  The effort for that project will be 737 person...                1.0   \n",
      "\n",
      "      faithfulness  answer_correctness  \n",
      "0              0.0            1.000000  \n",
      "1              1.0            0.746181  \n",
      "2              0.0            0.236377  \n",
      "3              0.0            0.239273  \n",
      "4              0.0            0.992143  \n",
      "...            ...                 ...  \n",
      "1348           1.0            0.740007  \n",
      "1349           0.0            0.237955  \n",
      "1350           0.0            1.000000  \n",
      "1351           0.0            1.000000  \n",
      "1352           0.0            0.234521  \n",
      "\n",
      "[1353 rows x 7 columns]\n",
      "\n",
      "Average Context Precision: 0.9760\n",
      "Average Faithfulness: 0.2917\n",
      "Average Answer Correctness: 0.7020\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "metrics = [context_precision, faithfulness, answer_correctness]\n",
    "\n",
    "print(\"Evaluating using RAGAS...\")\n",
    "scores = evaluate(dataset, metrics=metrics)\n",
    "\n",
    "# Save results\n",
    "scores_df = scores.to_pandas()\n",
    "scores_df.to_csv(\"evaluation_final_scores.csv\", index=False)\n",
    "\n",
    "# Print avg scores\n",
    "print(\"\\n Evaluation Scores:\")\n",
    "print(scores_df)\n",
    "\n",
    "print(f\"\\nAverage Context Precision: {scores_df['context_precision'].mean():.4f}\")\n",
    "print(f\"Average Faithfulness: {scores_df['faithfulness'].mean():.4f}\")\n",
    "print(f\"Average Answer Correctness: {scores_df['answer_correctness'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
